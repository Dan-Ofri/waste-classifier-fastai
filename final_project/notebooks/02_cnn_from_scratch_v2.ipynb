{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901863fb",
   "metadata": {},
   "source": [
    "# ğŸ§  Part 1: Building Optimized CNN from Scratch\n",
    "## ×’×¨×¡×” ××™×•×˜×‘×ª - ××”×™×¨×” + ×¤×—×•×ª overfitting!\n",
    "\n",
    "---\n",
    "\n",
    "### âœ¨ ×©×™×¤×•×¨×™× ×‘×’×¨×¡×” ×”×–×•:\n",
    "1. **××•×“×œ ×§×œ ×™×•×ª×¨** - 3 conv layers ××‘×œ ×¤×—×•×ª channels (16â†’32â†’64)\n",
    "2. **dropout ×—×–×§ ×™×•×ª×¨** - 0.6 ×‘××§×•× 0.5 ×œ×× ×™×¢×ª overfitting\n",
    "3. **×¤×—×•×ª epochs** - 25 ×‘××§×•× 50 (×—×™×¡×›×•×Ÿ ×–××Ÿ!)\n",
    "4. **Early Stopping** - ×¢×¦×™×¨×” ××•×˜×•××˜×™×ª ××—×¨×™ 5 epochs ×œ×œ× ×©×™×¤×•×¨\n",
    "5. **3 FC layers** - ××¢×‘×¨ ×”×“×¨×’×ª×™ ×™×•×ª×¨ (256â†’128â†’4)\n",
    "\n",
    "### ğŸ“Š ×ª×•×¦××•×ª ×¦×¤×•×™×•×ª:\n",
    "- â±ï¸ ×–××Ÿ ××™××•×Ÿ: ~20-30 ×“×§×•×ª (×‘××§×•× 2+ ×©×¢×•×ª!)\n",
    "- ğŸ“‰ ×¤×—×•×ª overfitting\n",
    "- ğŸ¯ accuracy ×“×•××” ××• ×˜×•×‘ ×™×•×ª×¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bbee9c",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ×™×™×‘×•× ×¡×¤×¨×™×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9875b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "import config\n",
    "\n",
    "print(\"âœ… All libraries imported!\")\n",
    "print(f\"ğŸ–¥ï¸  Device: {config.DEVICE}\")\n",
    "print(f\"ğŸ“¦ Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"ğŸ“ˆ Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"ğŸ”¢ Max Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"â¸ï¸  Early Stopping Patience: {config.PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4aef34",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ×˜×¢×™× ×ª Dataset Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82786a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_path = config.RESULTS_DIR / 'dataset_splits.json'\n",
    "\n",
    "with open(splits_path, 'r') as f:\n",
    "    splits_data = json.load(f)\n",
    "\n",
    "train_indices = splits_data['train_indices']\n",
    "val_indices = splits_data['val_indices']\n",
    "test_indices = splits_data['test_indices']\n",
    "class_names = splits_data['class_names']\n",
    "num_classes = splits_data['num_classes']\n",
    "\n",
    "print(\"âœ… Splits loaded!\")\n",
    "print(f\"ğŸ“Š Train: {len(train_indices)}, Val: {len(val_indices)}, Test: {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb096c9",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ×™×¦×™×¨×ª DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE + 32, config.IMAGE_SIZE + 32)),\n",
    "    transforms.RandomCrop((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "data_path = config.DATA_PATH\n",
    "\n",
    "train_dataset_full = ImageFolder(root=str(data_path), transform=train_transforms)\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "\n",
    "val_dataset_full = ImageFolder(root=str(data_path), transform=val_test_transforms)\n",
    "val_dataset = Subset(val_dataset_full, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"âœ… DataLoaders ready! Train: {len(train_loader)} batches, Val: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9782f0a",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ SimpleCNN - ×’×¨×¡×” ××™×•×˜×‘×ª! ğŸ—ï¸\n",
    "\n",
    "### ğŸ”¥ ×”×©×™× ×•×™×™×:\n",
    "- **Channels:** 16â†’32â†’64 (×‘××§×•× 32â†’64)\n",
    "- **Conv layers:** 3 ×‘××§×•× 2 (×™×•×ª×¨ ×¢×•××§, ×¤×—×•×ª ×¨×•×—×‘)\n",
    "- **FC layers:** 256â†’128â†’4 (×‘××§×•× 512â†’4)\n",
    "- **Dropout:** 0.6 ×•-0.5 (×‘××§×•× 0.5 ×‘×œ×‘×“)\n",
    "\n",
    "**×ª×•×¦××”:** ~12M parameters ×‘××§×•× 102M! âš¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8aedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: 3â†’16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 224â†’112\n",
    "        \n",
    "        # Block 2: 16â†’32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 112â†’56\n",
    "        \n",
    "        # Block 3: 32â†’64\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # 56â†’28\n",
    "        \n",
    "        # Flatten: 64Ã—28Ã—28 = 50,176\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # FC1: 50176â†’256\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.6)  # Dropout ×—×–×§\n",
    "        \n",
    "        # FC2: 256â†’128\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # FC3: 128â†’4\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv blocks\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))  # [B,3,224,224]â†’[B,16,112,112]\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))  # [B,16,112,112]â†’[B,32,56,56]\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))  # [B,32,56,56]â†’[B,64,28,28]\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.flatten(x)                        # [B,64,28,28]â†’[B,50176]\n",
    "        x = self.dropout1(self.relu4(self.fc1(x)))  # [B,50176]â†’[B,256]\n",
    "        x = self.dropout2(self.relu5(self.fc2(x)))  # [B,256]â†’[B,128]\n",
    "        x = self.fc3(x)                            # [B,128]â†’[B,4]\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = SimpleCNN(num_classes=num_classes).to(config.DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"âœ… SimpleCNN created!\")\n",
    "print(f\"ğŸ”¢ Total parameters: {total_params:,}\")\n",
    "print(f\"ğŸ’¾ Model size: ~{total_params * 4 / (1024**2):.1f} MB\")\n",
    "print(f\"\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a0c95",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(\"âœ… Optimizer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d73ea",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ ×¤×•× ×§×¦×™×•×ª Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation', leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "print(\"âœ… Training functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885f828",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ ×œ×•×œ××ª ×”××™××•×Ÿ + Early Stopping ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f15005",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_acc = 0.0\n",
    "best_model_path = config.MODELS_DIR / 'simple_cnn_optimized_best.pth'\n",
    "\n",
    "# Early Stopping\n",
    "patience = config.PATIENCE\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "print(f\"   Max Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"   Early Stopping: {patience} epochs patience\")\n",
    "print(f\"   Device: {config.DEVICE}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, config.DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, config.DEVICE)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Print\n",
    "    print(f\"Epoch [{epoch+1:2d}/{config.NUM_EPOCHS}] | \"\n",
    "          f\"Train: {train_loss:.4f} / {train_acc:5.2f}% | \"\n",
    "          f\"Val: {val_loss:.4f} / {val_acc:5.2f}% | \"\n",
    "          f\"{epoch_time:.1f}s\", end='')\n",
    "    \n",
    "    # Save best model & Early Stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\" ğŸ’¾ BEST!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\" â³ ({patience_counter}/{patience})\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n\\nâš ï¸  Early stopping after {epoch+1} epochs (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ… Training completed!\")\n",
    "print(f\"   Time: {total_time/60:.1f} minutes\")\n",
    "print(f\"   Best Val Acc: {best_val_acc:.2f}%\")\n",
    "print(f\"   Saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d4240",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-o', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-o', label='Val', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-o', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-o', label='Val', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.PLOTS_DIR / '06_optimized_cnn_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¾ Plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5610d",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ × ×™×ª×•×— ×ª×•×¦××•×ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_acc = history['train_acc'][-1]\n",
    "final_val_acc = history['val_acc'][-1]\n",
    "acc_gap = final_train_acc - final_val_acc\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š Final Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ¯ Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"ğŸ“ˆ Final Train Acc: {final_train_acc:.2f}%\")\n",
    "print(f\"ğŸ“‰ Final Val Acc: {final_val_acc:.2f}%\")\n",
    "print(f\"âš ï¸  Overfitting Gap: {acc_gap:.2f}%\")\n",
    "print(f\"â±ï¸  Training Time: {total_time/60:.1f} minutes\")\n",
    "print(f\"ğŸ”¢ Parameters: {total_params:,}\")\n",
    "\n",
    "if acc_gap < 5:\n",
    "    print(\"\\nâœ… Excellent! Minimal overfitting.\")\n",
    "elif acc_gap < 10:\n",
    "    print(\"\\nğŸ‘ Good! Some overfitting but acceptable.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Still overfitting. Consider more regularization.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da7c86",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ ×©××™×¨×ª ×ª×•×¦××•×ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'model_name': 'SimpleCNN_Optimized',\n",
    "    'num_epochs_ran': len(history['train_loss']),\n",
    "    'max_epochs': config.NUM_EPOCHS,\n",
    "    'batch_size': config.BATCH_SIZE,\n",
    "    'learning_rate': config.LEARNING_RATE,\n",
    "    'early_stopping_patience': patience,\n",
    "    'best_val_acc': float(best_val_acc),\n",
    "    'final_train_acc': float(final_train_acc),\n",
    "    'final_val_acc': float(final_val_acc),\n",
    "    'overfitting_gap': float(acc_gap),\n",
    "    'total_params': total_params,\n",
    "    'training_time_minutes': total_time / 60,\n",
    "    'history': history\n",
    "}\n",
    "\n",
    "results_path = config.LOGS_DIR / 'simple_cnn_optimized_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"ğŸ’¾ Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd1444",
   "metadata": {},
   "source": [
    "## ğŸ“ ×¡×™×›×•×\n",
    "\n",
    "### âœ¨ ××” ×©×™×¤×¨× ×•:\n",
    "1. **××”×™×¨×•×ª:** ×¤×—×•×ª ×¤×¨××˜×¨×™× = ××™××•×Ÿ ××”×¨ ×™×•×ª×¨\n",
    "2. **Overfitting:** Dropout ×—×–×§ + Early stopping\n",
    "3. **×™×¢×™×œ×•×ª:** ×¢×¦×™×¨×” ××•×˜×•××˜×™×ª ×›×©×œ× ××©×ª×¤×¨\n",
    "\n",
    "### ğŸš€ ×”×¦×¢×“ ×”×‘×:\n",
    "×‘××—×‘×¨×ª ×”×‘××” × ×•×¡×™×£ **Batch Normalization** ×œ×©×™×¤×•×¨ × ×•×¡×£!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
